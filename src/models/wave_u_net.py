import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint_sequential

"""
We use the same VCTK dataset

ADAM optimization algorithm, a learning rate of 0.0001, decay rates β1 = 0.9 and β2 = 0.999
a batch size of 16.

We specify an initial network layer size of 12
16 extra filters per layer are also specified,
with downsampling block filters of size 15 and upsampling block filters of size 5 like in [12].

We train for 2,000 iterations with mean squared error (MSE) over all source output samples in a batch as
loss and apply early stopping if there is no improvement on the validation set for 20 epochs.

We use a fixed validation set of 10 randomly selected tracks. 

Then, the best model is fine-tuned with the batch size doubled and 
the learning rate lowered to 0.00001, again until 20 epochs have passed without
improved validation loss.

Under our implementation, training took c.36 hours using GeForce GTX 1080 Ti GPU with 11178 MiB

Train and test datasets provided by the 28-speaker 
[Voice Bank Corpus (VCTK)](https://datashare.is.ed.ac.uk/handle/10283/2791) [4]
(30 speakers in total - 28 intended for training and 2 reserved for testing).
 The noisy training data were generated by mixing the clean data with various noise datasets, 
 as per the instructions provided in [4, 5, 6].



audio-based MSE loss and mono
signals downsampled to 8192 Hz

look into https://github.com/pytorch/audio
for
    data loading
    audio transforms
    resampling
"""


class WaveUNet(nn.Module):
    """
    Convolutional neural net for speech enhancement
    Proposed in Improved Speech Enhancement with the Wave-U-Net (https://arxiv.org/pdf/1811.11307.pdf),
    which was in turn inspired by this paper (https://arxiv.org/pdf/1806.03185.pdf)
    """

    def __init__(self):
        super().__init__()
        # Construct downsampling sequence
        # out = in - filter + 1
        self.ds_conv_1 = DownSampleConvLayer(1, 1 * 24)
        self.ds_conv_2 = DownSampleConvLayer(1 * 24, 2 * 24)
        self.ds_conv_3 = DownSampleConvLayer(2 * 24, 3 * 24)
        self.ds_conv_4 = DownSampleConvLayer(3 * 24, 4 * 24)
        self.ds_conv_5 = DownSampleConvLayer(4 * 24, 5 * 24)
        self.ds_conv_6 = DownSampleConvLayer(5 * 24, 6 * 24)
        self.ds_conv_7 = DownSampleConvLayer(6 * 24, 7 * 24)
        self.ds_conv_8 = DownSampleConvLayer(7 * 24, 8 * 24)
        self.ds_conv_9 = DownSampleConvLayer(8 * 24, 9 * 24)
        self.ds_conv_10 = DownSampleConvLayer(9 * 24, 10 * 24)
        self.ds_conv_11 = DownSampleConvLayer(10 * 24, 11 * 24)
        self.ds_conv_12 = DownSampleConvLayer(11 * 24, 12 * 24)

        self.md_conv = MiddleConvLayer(12 * 24, 13 * 24)

        self.us_conv_1 = UpSampleConvLayer((2 * 13 - 1) * 24, 12 * 24)
        self.us_conv_2 = UpSampleConvLayer((2 * 12 - 1) * 24, 11 * 24)
        self.us_conv_3 = UpSampleConvLayer((2 * 11 - 1) * 24, 10 * 24)
        self.us_conv_4 = UpSampleConvLayer((2 * 10 - 1) * 24, 9 * 24)
        self.us_conv_5 = UpSampleConvLayer((2 * 9 - 1) * 24, 8 * 24)
        self.us_conv_6 = UpSampleConvLayer((2 * 8 - 1) * 24, 7 * 24)
        self.us_conv_7 = UpSampleConvLayer((2 * 7 - 1) * 24, 6 * 24)
        self.us_conv_8 = UpSampleConvLayer((2 * 6 - 1) * 24, 5 * 24)
        self.us_conv_9 = UpSampleConvLayer((2 * 5 - 1) * 24, 4 * 24)
        self.us_conv_10 = UpSampleConvLayer((2 * 4 - 1) * 24, 3 * 24)
        self.us_conv_11 = UpSampleConvLayer((2 * 3 - 1) * 24, 2 * 24)
        self.us_conv_12 = UpSampleConvLayer((2 * 2 - 1) * 24, 1 * 24)

        self.out_conv = OutputConvLayer(24, 1)

    def forward(self, input_t):
        """
        Input has shape (b, 1, audio_length = 16384+,) 
        Output has shape (b, 1, audio_length = ???,) 

        Fc = 24 extra filters
        per layer and filter sizes fd = 15 and fu = 5
        """
        # Downsampling
        # (b, 1, 16384)
        acts, ds_acts_1 = self.ds_conv_1(input_t)
        # (b, 24, 8192), (b, 24, 16384)
        acts, ds_acts_2 = self.ds_conv_2(acts)
        # (b, 48, 4096), (b, 48, 8192)
        acts, ds_acts_3 = self.ds_conv_3(acts)
        # (b, 72, 2048), (b, 72, 4096)
        acts, ds_acts_4 = self.ds_conv_4(acts)
        # (b, 96, 1024), (b, 96, 2048)
        acts, ds_acts_5 = self.ds_conv_5(acts)
        # (b, 120, 512), (b, 120, 1024)
        acts, ds_acts_6 = self.ds_conv_6(acts)
        # (b, 144, 256), (b, 144, 512)
        acts, ds_acts_7 = self.ds_conv_7(acts)
        # (b, 168, 128), (b, 168, 256)
        acts, ds_acts_8 = self.ds_conv_8(acts)
        # (b, 192, 64), (b, 192, 128)
        acts, ds_acts_9 = self.ds_conv_9(acts)
        # (b, 216, 32), (b, 216, 64)
        acts, ds_acts_10 = self.ds_conv_10(acts)
        # (b, 240, 16), (b, 240, 32)
        acts, ds_acts_11 = self.ds_conv_11(acts)
        # (b, 264, 8), (b, 264, 16)
        acts, ds_acts_12 = self.ds_conv_12(acts)
        # (b, 288, 4), (b, 288, 8)
        acts = self.md_conv(acts)
        # (b, 312, 4)

        # Upsampling
        acts = self.us_conv_1(acts, ds_acts_12)
        # (b, 288, 8)
        acts = self.us_conv_2(acts, ds_acts_11)
        # (b, 264, 16)
        acts = self.us_conv_3(acts, ds_acts_10)
        # (b, 240, 32)
        acts = self.us_conv_4(acts, ds_acts_9)
        # (b, 216, 64)
        acts = self.us_conv_5(acts, ds_acts_8)
        # (b, 192, 128)
        acts = self.us_conv_6(acts, ds_acts_7)
        # (b, 168, 256)
        acts = self.us_conv_7(acts, ds_acts_6)
        # (b, 144, 512)
        acts = self.us_conv_8(acts, ds_acts_5)
        # (b, 120, 1024)
        acts = self.us_conv_9(acts, ds_acts_4)
        # (b, 96, 2048)
        acts = self.us_conv_10(acts, ds_acts_3)
        # (b, 72, 4096)
        acts = self.us_conv_11(acts, ds_acts_2)
        # (b, 48, 8192)
        acts = self.us_conv_12(acts, ds_acts_1)
        # (b, 24, 16384)
        output_t = self.out_conv(acts, input_t)
        # (batch, 1, 16384) (or 1, 3, 5, etc.)
        return output_t


class DownSampleConvLayer(nn.Module):
    """
    Single convolutional layer for downsampling
    """

    def __init__(self, in_channels, out_channels):
        """
        Setup the layer.
            in_channels: number of input channels to be convoluted
            out_channels: number of output channels to be produced

        """
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            # Same padding
            padding=7,
            kernel_size=15,
            bias=True,
        )
        # Apply Kaiming initialization to convolutional weights
        nn.init.xavier_uniform_(self.conv.weight)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)

    def forward(self, input_t):
        """
        Compute output tensor from input tensor
        """
        conv_t = self.conv(input_t)
        relu_t = self.leaky_relu(conv_t)
        # Decimate discards features for every other time step to halve the time resolution
        decimated_t = relu_t[:, :, ::2]
        return decimated_t, relu_t


class MiddleConvLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        """
        Setup the layer.
            in_channels: number of input channels to be convoluted
            out_channels: number of output channels to be produced

        """
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            # Same padding
            padding=7,
            kernel_size=15,
            bias=True,
        )
        # Apply Kaiming initialization to convolutional weights
        nn.init.xavier_uniform_(self.conv.weight)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)

    def forward(self, input_t):
        """
        Compute output tensor from input tensor
        """
        conv_t = self.conv(input_t)
        relu_t = self.leaky_relu(conv_t)
        return relu_t


class UpSampleConvLayer(nn.Module):
    """
    Single convolutional layer for upsampling
    """

    def __init__(self, in_channels, out_channels):
        """
        Setup the layer.
            in_channels: number of input channels to be convoluted
            out_channels: number of output channels to be produced

        """
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=5,
            # Same padding
            padding=2,
            bias=True,
        )
        # Apply Kaiming initialization to convolutional weights
        nn.init.xavier_uniform_(self.conv.weight)
        self.upsample = nn.Upsample(scale_factor=2, mode="linear", align_corners=True)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)

    def forward(self, input_t, sister_t):
        """
        Compute output tensor from input tensor
            input  (b, C + 24, L)
            sister (b, C, 2L)
        """
        # Upsample in the time direction by a factor of two, using interpolation
        # (b, 312, 4)
        upsampled_t = self.upsample(input_t)
        # (b, 312, 8)

        # Concatenate upsampled input and sister tensor from downsampling.
        # Perform the concatenation in the feature map dimension.
        # (b, 312, 8) + (b, 288, 8)
        combined_t = torch.cat((upsampled_t, sister_t), dim=1)

        # Run combined feature maps through convolutional layer.
        # (b, 600, 8)
        conv_t = self.conv(combined_t)
        relu_t = self.leaky_relu(conv_t)
        return relu_t


class OutputConvLayer(nn.Module):
    """
    Single convolutional layer for output
    """

    def __init__(self, in_channels, out_channels):
        """
        Setup the layer.
            in_channels: number of input channels to be convoluted
            out_channels: number of output channels to be produced

        """
        super().__init__()
        self.conv = nn.Conv1d(
            in_channels=in_channels + 1, out_channels=out_channels, kernel_size=1, bias=True
        )
        # Apply Kaiming initialization to convolutional weights
        nn.init.xavier_uniform_(self.conv.weight)
        self.tanh = nn.Tanh()

    def forward(self, input_t, sister_t):
        """
        Compute output tensor from input tensor
        """
        # Concatenate upsampled feature maps and input tensor.
        # Perform the concatenation in the feature map dimension.
        combined_t = torch.cat((input_t, sister_t), dim=1)

        # Run combined feature maps through convolutional layer.
        conv_t = self.conv(combined_t)
        output_t = self.tanh(conv_t)
        return output_t
